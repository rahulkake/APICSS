import os
import time
import json
import mlflow
import logging

# Set the MLflow tracking URI to 'http'
mlflow.set_tracking_uri("http://localhost:5000")

logging.getLogger("mlflow").setLevel(logging.WARNING)


def log_metrics(question, response_text, start_time, model_name):
    """
    Logs metrics for a model response.

    Parameters:
    question (str): The question that was asked.
    response_text (str): The response generated by the model.
    start_time (float): The time when the request started.
    model_name (str): The name of the model used.
    """

    # Calculate metrics
    latency = time.time() - start_time
    question_token_count = len(question.split())
    response_token_count = len(response_text.split())
    response_length = len(response_text)
    estimated_cost = (question_token_count + response_token_count) * 0.00001  # hypothetical cost per token

    # Log metrics and parameters
    mlflow.log_param("model", model_name)
    mlflow.log_param("question", question)
    mlflow.log_metric("latency", latency)
    mlflow.log_metric("question_token_count", question_token_count)
    mlflow.log_metric("response_token_count", response_token_count)
    mlflow.log_metric("response_length", response_length)
    mlflow.log_metric("estimated_cost", estimated_cost)
    mlflow.log_text(response_text, "response_text.txt")

    # Ensure the logs directory exists
    os.makedirs("mlflow_logs", exist_ok=True)

    # Save logs to a local JSON file
    log_data = {
        "model": model_name,
        "question": question,
        "latency": latency,
        "question_token_count": question_token_count,
        "response_token_count": response_token_count,
        "response_length": response_length,
        "estimated_cost": estimated_cost,
        "response_text": response_text
    }

    model_name_safe = model_name.replace("/", "_")
    with open(f"mlflow_logs/log_{model_name_safe}_{time.time()}.json", "w") as log_file:
        json.dump(log_data, log_file)